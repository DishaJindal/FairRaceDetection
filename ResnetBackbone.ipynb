{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "680_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpI2z24T5g5O",
        "colab_type": "text"
      },
      "source": [
        "# Basic Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHOch_tj4YMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext tensorboard\n",
        "import os, shutil\n",
        "import time, datetime\n",
        "\n",
        "def delete_logs():\n",
        "  folder = \"./logs\"\n",
        "  for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "          if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "              os.unlink(file_path)\n",
        "          elif os.path.isdir(file_path):\n",
        "              shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "          print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "delete_logs()\n",
        "def get_tensorboard_logger():\n",
        "  logs_base_dir = \"./logs\"\n",
        "  logs_dir = logs_base_dir + \"/run_\" + str(time.mktime(datetime.datetime.now().timetuple()))\n",
        "  os.makedirs(logs_dir, exist_ok=True)\n",
        "  from torch.utils.tensorboard import SummaryWriter\n",
        "  %tensorboard --logdir {logs_base_dir} --port=8998\n",
        "  logger = SummaryWriter(logs_dir)\n",
        "  return logger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD8-La5r5uwL",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C74DohZD57YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchvision.datasets import MNIST\n",
        "# from torchvision import datasets\n",
        "# from torchvision import transforms\n",
        "\n",
        "# batch_size = 50\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#         datasets.MNIST('../data', train=True, download=True,\n",
        "#                        transform=transforms.Compose([\n",
        "#                            transforms.ToTensor(),\n",
        "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                        ])),\n",
        "#         batch_size=batch_size, shuffle=True )\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#         datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                            transforms.ToTensor(),\n",
        "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                        ])),\n",
        "#         batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKFgNKd5Ihvn",
        "colab_type": "text"
      },
      "source": [
        "# Cluttered MNIST\n",
        "https://github.com/kvn219/cluttered-mnist/blob/master/spatial-transformer-network/Clutter_MNIST_Example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy624Fz9HFGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CTrainDataset():\n",
        "  def __init__(self, input_path):    \n",
        "    mnist_cluttered = np.load(input_path)\n",
        "    self.X_train = mnist_cluttered['X_train']\n",
        "    self.y_train = mnist_cluttered['y_train']\n",
        "    self.length = len(self.X_train)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.length \n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.X_train[index].reshape((1, 40, 40)), self.y_train[index][0]\n",
        "\n",
        "def custom_collate(batch):\n",
        "  return default_collate(batch)\n",
        "\n",
        "class CTestDataset():\n",
        "  def __init__(self, input_path):    \n",
        "    mnist_cluttered = np.load(input_path)\n",
        "    self.X_test = mnist_cluttered['X_test']\n",
        "    self.y_test = mnist_cluttered['y_test']\n",
        "    self.length = len(self.X_test)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.length \n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.X_test[index].reshape((1, 40, 40)), self.y_test[index][0]\n",
        "\n",
        "def custom_collate(batch):\n",
        "  return default_collate(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKIoBoUZHQL3",
        "colab_type": "code",
        "outputId": "919c1e5a-e849-4b0d-cb01-5607c2fec0c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "batch_size = 1000\n",
        "train_dataset = CTrainDataset('mnist_sequence1_sample_5distortions5x5.npz')\n",
        "test_dataset = CTestDataset('mnist_sequence1_sample_5distortions5x5.npz')\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False )\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False )\n",
        "\n",
        "idx = 6\n",
        "img, y = train_dataset[idx]\n",
        "img = img[0]\n",
        "print(np.max(img))\n",
        "print(y)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.99609375\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHE-FYypjfJi",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv0s86ahjfmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_norm(norm_type, num_features, num_groups=32, eps=1e-5):\n",
        "  if norm_type == 'BatchNorm':\n",
        "    return nn.BatchNorm2d(num_features, eps=eps)\n",
        "  elif norm_type == \"GroupNorm\":\n",
        "    return nn.GroupNorm(num_groups, num_features, eps=eps)\n",
        "  elif norm_type == \"InstanceNorm\":\n",
        "    return nn.InstanceNorm2d(num_features, eps=eps, affine=True, track_running_stats=True)\n",
        "  else:\n",
        "    raise Exception('Unknown Norm Function : {}'.format(norm_type))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvnue0UVjjNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Conv2dNormRelu(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=0, bias=True, norm_type='Unknown'):\n",
        "    super(Conv2dNormRelu, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias),\n",
        "        get_norm(norm_type, out_ch),\n",
        "        nn.ReLU(inplace=True))\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class CAModule(nn.Module):\n",
        "  def __init__(self, num_channels, reduc_ratio=2):\n",
        "    super(CAModule, self).__init__()\n",
        "    self.num_channels = num_channels\n",
        "    self.reduc_ratio = reduc_ratio\n",
        "    self.fc1 = nn.Linear(num_channels, num_channels // reduc_ratio, bias=True)\n",
        "    self.fc2 = nn.Linear(num_channels // reduc_ratio, num_channels, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  def forward(self, feat_map):\n",
        "    gap_out = feat_map.view(feat_map.size()[0], self.num_channels, -1).mean(dim=2)\n",
        "    fc1_out = self.relu(self.fc1(gap_out))\n",
        "    fc2_out = self.sigmoid(self.fc2(fc1_out))\n",
        "    fc2_out = fc2_out.view(fc2_out.size()[0], fc2_out.size()[1], 1, 1)\n",
        "    feat_map = torch.mul(feat_map, fc2_out)\n",
        "    return feat_map, [fc2_out]\n",
        "\n",
        "class SAModule(nn.Module):\n",
        "  def __init__(self, num_channels):\n",
        "    super(SAModule, self).__init__()\n",
        "    self.num_channels = num_channels\n",
        "    self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels // 8, kernel_size=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels // 8, kernel_size=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=1)\n",
        "    self.gamma = nn.Parameter(torch.zeros(1))\n",
        "  \n",
        "  def forward(self, feat_map):\n",
        "    batch_size, num_channels, height, width = feat_map.size()\n",
        "    conv1_proj = self.conv1(feat_map).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "    conv2_proj = self.conv2(feat_map).view(batch_size, -1, width * height)\n",
        "    relation_map = torch.bmm(conv1_proj, conv2_proj)\n",
        "    attention = F.softmax(relation_map, dim=-1)\n",
        "    conv3_proj = self.conv3(feat_map).view(batch_size, -1, width * height)\n",
        "    feat_refine = torch.bmm(conv3_proj, attention.permute(0, 2, 1))\n",
        "    feat_refine = feat_refine.view(batch_size, num_channels, height, width)\n",
        "    feat_map = self.gamma * feat_refine + feat_map\n",
        "    return feat_map, [self.gamma * feat_refine, feat_refine]\n",
        "\n",
        "\n",
        "class FPAModule(nn.Module):\n",
        "  def __init__(self, num_channels, norm_type):\n",
        "    super(FPAModule, self).__init__()\n",
        "    self.gap_branch = nn.Sequential(nn.AdaptiveAvgPool2d(1), Conv2dNormRelu(num_channels, num_channels, kernel_size=1, norm_type=norm_type))\n",
        "    self.mid_branch = Conv2dNormRelu(num_channels, num_channels, kernel_size=1, norm_type=norm_type)\n",
        "    self.downsample1 = Conv2dNormRelu(num_channels, 1, kernel_size=3, stride=2, padding=1, norm_type=norm_type)\n",
        "    self.downsample2 = Conv2dNormRelu(1, 1, kernel_size=3, stride=2, padding=1, norm_type=norm_type)\n",
        "    # self.downsample3 = Conv2dNormRelu(1, 1, kernel_size=3, stride=2, padding=1, norm_type=norm_type)\n",
        "    self.scale1 = Conv2dNormRelu(1, 1, kernel_size=3, padding=1, norm_type=norm_type)\n",
        "    self.scale2 = Conv2dNormRelu(1, 1, kernel_size=3, padding=1, norm_type=norm_type)\n",
        "    # self.scale3 = Conv2dNormRelu(1, 1, kernel_size=3, padding=1, norm_type=norm_type)\n",
        "  \n",
        "  def forward(self, feat_map):\n",
        "    height, width = feat_map.size(2), feat_map.size(3)\n",
        "    gap_branch = self.gap_branch(feat_map)\n",
        "    gap_branch = nn.Upsample(size=(height, width), mode='bilinear', align_corners=False)(gap_branch)\n",
        "    mid_branch = self.mid_branch(feat_map)\n",
        "    scale1 = self.downsample1(feat_map)\n",
        "    scale2 = self.downsample2(scale1)\n",
        "    # scale3 = self.downsample3(scale2)\n",
        "    # scale3 = self.scale3(scale3)\n",
        "    # scale3 = nn.Upsample(size=(int(math.ceil(height*1.0 / 4.0)), int(math.ceil(width*1.0 / 4.0))), mode='bilinear', align_corners=False)(scale3)\n",
        "    # scale2 = self.scale2(scale2) + scale3\n",
        "    scale2 = self.scale2(scale2)\n",
        "    scale2 = nn.Upsample(size=(int(math.ceil(height*1.0 / 2.0)), int(math.ceil(width*1.0 / 2.0))), mode='bilinear', align_corners=False)(scale2)\n",
        "    scale1 = self.scale1(scale1) + scale2\n",
        "    scale1 = nn.Upsample(size=(height, width), mode='bilinear', align_corners=False)(scale1)\n",
        "    feat_map = torch.mul(scale1, mid_branch) + gap_branch\n",
        "    return feat_map, []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTbZ0MF-jqwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "3a1858b5-0018-4c9e-ef2e-9c51f10ffd89"
      },
      "source": [
        "from torchvision.models.utils import load_state_dict_from_url\n",
        "from torchvision.models.resnet import BasicBlock, _resnet\n",
        "import torch.nn as nn \n",
        "\n",
        "global_batchid = -1\n",
        "global_epoch = -1\n",
        "global_id = 0\n",
        "global_logger = get_tensorboard_logger()\n",
        "\n",
        "class AdvancedBasicBlock(BasicBlock):\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "    super(AdvancedBasicBlock, self).__init__(inplanes, planes, stride=stride, downsample=downsample, groups=groups,\n",
        "                 base_width=base_width, dilation=dilation, norm_layer=norm_layer)\n",
        "    \n",
        "    print(self.bn2.num_features)\n",
        "    global global_id\n",
        "    self.id = global_id\n",
        "    global_id += 1\n",
        "    self.attention_layer = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    \n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out, plot_data = self.attention_layer(out)\n",
        "    \n",
        "    global global_batchid, global_epoch, global_logger\n",
        "\n",
        "    local_batch_id = global_batchid\n",
        "    local_epoch = global_epoch\n",
        "\n",
        "    if  not self.training and local_batch_id < 1:\n",
        "      for index, value in enumerate(plot_data):\n",
        "        for channel_id in range(value.shape[1]):\n",
        "          key = str(self.id) + \"_\" + str(local_batch_id) + \"_\" + str(channel_id)\n",
        "          sample = value[:, channel_id, :, :].unsqueeze(1)\n",
        "          # sample = sample.repeat(1, 3, 1, 1)\n",
        "          global_logger.add_images(key, vutils.make_grid(sample.cpu(), padding=2).unsqueeze(0),  local_epoch)\n",
        "          \n",
        "    \n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "    \n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "    return out\n",
        "\n",
        "class AdvancedBasicBlock_CAM(AdvancedBasicBlock):\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, \n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(AdvancedBasicBlock_CAM, self).__init__(inplanes, planes, stride=stride, downsample=downsample, groups=groups,\n",
        "                 base_width=base_width, dilation=dilation, norm_layer=norm_layer)\n",
        "    self.attention_layer = CAModule(self.bn2.num_features)\n",
        "\n",
        "class AdvancedBasicBlock_SAM(AdvancedBasicBlock):\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, \n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(AdvancedBasicBlock_SAM, self).__init__(inplanes, planes, stride=stride, downsample=downsample, groups=groups,\n",
        "                 base_width=base_width, dilation=dilation, norm_layer=norm_layer)\n",
        "    self.attention_layer = SAModule(self.bn2.num_features)\n",
        "\n",
        "class AdvancedBasicBlock_FPA(AdvancedBasicBlock):\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, \n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(AdvancedBasicBlock_FPA, self).__init__(inplanes, planes, stride=stride, downsample=downsample, groups=groups,\n",
        "                 base_width=base_width, dilation=dilation, norm_layer=norm_layer)\n",
        "    self.attention_layer = FPAModule(self.bn2.num_features, 'BatchNorm')\n",
        "\n",
        "    \n",
        "def newResnet34(blk, pretrained=False, progress=True, **kwargs):\n",
        "  return _resnet('resnet34', blk, [3, 4, 6, 3], pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 8998 (pid 3460), started 0:14:35 ago. (Use '!kill 3460' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-b6d710378f81c287\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-b6d710378f81c287\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 8998;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWv-LGycHCiv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKpazWkJ5vF3",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeKFfcTA58aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network():\n",
        "  def __init__(self, bs=50, lr=0.001, epochs=10, model=None, mode='default'):\n",
        "    self.lr = lr\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = bs\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.kwargs = {'num_workers': 0, 'pin_memory': True} \n",
        "    if model is not None:\n",
        "      self.model = model.to(self.device)\n",
        "    else:\n",
        "      if mode == 'default':\n",
        "        resnet = torchvision.models.resnet34(pretrained=True)\n",
        "      elif mode == 'sam':\n",
        "        resnet = newResnet34(AdvancedBasicBlock_SAM, pretrained=False)\n",
        "        k = load_state_dict_from_url('https://download.pytorch.org/models/resnet34-333f7ec4.pth')\n",
        "        resnet.load_state_dict(k, strict=False)\n",
        "      elif mode == 'cam':\n",
        "        resnet = newResnet34(AdvancedBasicBlock_CAM, pretrained=False)\n",
        "        k = load_state_dict_from_url('https://download.pytorch.org/models/resnet34-333f7ec4.pth')\n",
        "        resnet.load_state_dict(k, strict=False)\n",
        "      elif mode == 'fpa':\n",
        "        resnet = newResnet34(AdvancedBasicBlock_FPA, pretrained=False)\n",
        "        k = load_state_dict_from_url('https://download.pytorch.org/models/resnet34-333f7ec4.pth')\n",
        "        resnet.load_state_dict(k, strict=False)\n",
        "\n",
        "      resnet.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
        "      resnet.fc = nn.Linear(512, 10)\n",
        "      self.model = resnet.to(self.device)\n",
        "      model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "      params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "      print(\"Number of Parameters: \", params)\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    global global_logger\n",
        "    self.logger = global_logger\n",
        "\n",
        "  def train(self, train_loader, test_loader):\n",
        "    self.model.train()\n",
        "    for epoch in range(self.epochs):\n",
        "      self.model.train()\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          data, target = data.to(self.device), target.to(self.device).long()\n",
        "          self.optimizer.zero_grad()\n",
        "          output = self.model(data)\n",
        "          loss = self.criterion(output, target)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.logger.add_scalar('Train_Loss', loss.item(), epoch*len(train_loader) + batch_idx)\n",
        "          if batch_idx % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "      self.test(epoch, test_loader)  \n",
        "\n",
        "  def test(self, epoch, test_loader):\n",
        "    self.model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    global global_epoch, global_batchid\n",
        "    global_epoch = epoch \n",
        "    with torch.no_grad():\n",
        "        for batch_id, tup in enumerate(test_loader):\n",
        "            global_batchid = batch_id\n",
        "            data, target = tup\n",
        "            data, target = data.to(self.device), target.to(self.device).long()\n",
        "            # plt.imshow(data[0].transpose(0,1).transpose(1,2).squeeze())\n",
        "            # plt.show()\n",
        "            output = self.model(data)\n",
        "            test_loss += self.criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            if(batch_id == 0):\n",
        "              global_logger.add_images(\"Input Image\", vutils.make_grid(data.cpu(), padding=2).unsqueeze(0),  epoch)\n",
        "    test_loss /= len(test_loader)\n",
        "    self.logger.add_scalar('Test_Loss', test_loss, epoch)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQIXGTUWKxNb",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj3gp94K9hnf",
        "colab_type": "code",
        "outputId": "725ad7cb-cfa6-4feb-feb7-eced3f6cc7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "resnet = Network(bs=batch_size, epochs=50, lr=0.001, mode='sam')\n",
        "resnet.train(train_loader=train_loader, test_loader=test_loader)\n",
        "\n",
        "# print(resnet.model)\n",
        "# from torchsummary import summary\n",
        "# summary(resnet.model, input_size=(1, 40, 40))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "64\n",
            "64\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "512\n",
            "512\n",
            "512\n",
            "Number of Parameters:  22860106\n",
            "Train Epoch: 0 [0/10000 (0%)]\tLoss: 2.670158\n",
            "\n",
            "Test set: Average loss: 7.3223, Accuracy: 224/1000 (2240%)\n",
            "\n",
            "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.677372\n",
            "\n",
            "Test set: Average loss: 9.2118, Accuracy: 204/1000 (2040%)\n",
            "\n",
            "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.311813\n",
            "\n",
            "Test set: Average loss: 6.9171, Accuracy: 250/1000 (2500%)\n",
            "\n",
            "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.191767\n",
            "\n",
            "Test set: Average loss: 5.3091, Accuracy: 303/1000 (3030%)\n",
            "\n",
            "Train Epoch: 4 [0/10000 (0%)]\tLoss: 0.116164\n",
            "\n",
            "Test set: Average loss: 0.9152, Accuracy: 789/1000 (7890%)\n",
            "\n",
            "Train Epoch: 5 [0/10000 (0%)]\tLoss: 0.083475\n",
            "\n",
            "Test set: Average loss: 0.5644, Accuracy: 870/1000 (8700%)\n",
            "\n",
            "Train Epoch: 6 [0/10000 (0%)]\tLoss: 0.056215\n",
            "\n",
            "Test set: Average loss: 0.8212, Accuracy: 815/1000 (8150%)\n",
            "\n",
            "Train Epoch: 7 [0/10000 (0%)]\tLoss: 0.066211\n",
            "\n",
            "Test set: Average loss: 0.6334, Accuracy: 860/1000 (8600%)\n",
            "\n",
            "Train Epoch: 8 [0/10000 (0%)]\tLoss: 0.074346\n",
            "\n",
            "Test set: Average loss: 0.8384, Accuracy: 837/1000 (8370%)\n",
            "\n",
            "Train Epoch: 9 [0/10000 (0%)]\tLoss: 0.037720\n",
            "\n",
            "Test set: Average loss: 0.3156, Accuracy: 922/1000 (9220%)\n",
            "\n",
            "Train Epoch: 10 [0/10000 (0%)]\tLoss: 0.062594\n",
            "\n",
            "Test set: Average loss: 0.4282, Accuracy: 914/1000 (9140%)\n",
            "\n",
            "Train Epoch: 11 [0/10000 (0%)]\tLoss: 0.025954\n",
            "\n",
            "Test set: Average loss: 0.3311, Accuracy: 920/1000 (9200%)\n",
            "\n",
            "Train Epoch: 12 [0/10000 (0%)]\tLoss: 0.018874\n",
            "\n",
            "Test set: Average loss: 0.2165, Accuracy: 949/1000 (9490%)\n",
            "\n",
            "Train Epoch: 13 [0/10000 (0%)]\tLoss: 0.006410\n",
            "\n",
            "Test set: Average loss: 0.2526, Accuracy: 940/1000 (9400%)\n",
            "\n",
            "Train Epoch: 14 [0/10000 (0%)]\tLoss: 0.009136\n",
            "\n",
            "Test set: Average loss: 0.2080, Accuracy: 954/1000 (9540%)\n",
            "\n",
            "Train Epoch: 15 [0/10000 (0%)]\tLoss: 0.006270\n",
            "\n",
            "Test set: Average loss: 0.2322, Accuracy: 951/1000 (9510%)\n",
            "\n",
            "Train Epoch: 16 [0/10000 (0%)]\tLoss: 0.003328\n",
            "\n",
            "Test set: Average loss: 0.2660, Accuracy: 941/1000 (9410%)\n",
            "\n",
            "Train Epoch: 17 [0/10000 (0%)]\tLoss: 0.002841\n",
            "\n",
            "Test set: Average loss: 0.2132, Accuracy: 954/1000 (9540%)\n",
            "\n",
            "Train Epoch: 18 [0/10000 (0%)]\tLoss: 0.000491\n",
            "\n",
            "Test set: Average loss: 0.2361, Accuracy: 951/1000 (9510%)\n",
            "\n",
            "Train Epoch: 19 [0/10000 (0%)]\tLoss: 0.002404\n",
            "\n",
            "Test set: Average loss: 0.2100, Accuracy: 955/1000 (9550%)\n",
            "\n",
            "Train Epoch: 20 [0/10000 (0%)]\tLoss: 0.000297\n",
            "\n",
            "Test set: Average loss: 0.2080, Accuracy: 954/1000 (9540%)\n",
            "\n",
            "Train Epoch: 21 [0/10000 (0%)]\tLoss: 0.000187\n",
            "\n",
            "Test set: Average loss: 0.2066, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 22 [0/10000 (0%)]\tLoss: 0.000371\n",
            "\n",
            "Test set: Average loss: 0.2187, Accuracy: 956/1000 (9560%)\n",
            "\n",
            "Train Epoch: 23 [0/10000 (0%)]\tLoss: 0.000147\n",
            "\n",
            "Test set: Average loss: 0.2226, Accuracy: 957/1000 (9570%)\n",
            "\n",
            "Train Epoch: 24 [0/10000 (0%)]\tLoss: 0.000128\n",
            "\n",
            "Test set: Average loss: 0.2222, Accuracy: 955/1000 (9550%)\n",
            "\n",
            "Train Epoch: 25 [0/10000 (0%)]\tLoss: 0.000108\n",
            "\n",
            "Test set: Average loss: 0.2217, Accuracy: 956/1000 (9560%)\n",
            "\n",
            "Train Epoch: 26 [0/10000 (0%)]\tLoss: 0.000092\n",
            "\n",
            "Test set: Average loss: 0.2215, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 27 [0/10000 (0%)]\tLoss: 0.000081\n",
            "\n",
            "Test set: Average loss: 0.2216, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 28 [0/10000 (0%)]\tLoss: 0.000072\n",
            "\n",
            "Test set: Average loss: 0.2219, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 29 [0/10000 (0%)]\tLoss: 0.000065\n",
            "\n",
            "Test set: Average loss: 0.2224, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 30 [0/10000 (0%)]\tLoss: 0.000060\n",
            "\n",
            "Test set: Average loss: 0.2228, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 31 [0/10000 (0%)]\tLoss: 0.000055\n",
            "\n",
            "Test set: Average loss: 0.2232, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 32 [0/10000 (0%)]\tLoss: 0.000051\n",
            "\n",
            "Test set: Average loss: 0.2237, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 33 [0/10000 (0%)]\tLoss: 0.000048\n",
            "\n",
            "Test set: Average loss: 0.2241, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 34 [0/10000 (0%)]\tLoss: 0.000045\n",
            "\n",
            "Test set: Average loss: 0.2246, Accuracy: 957/1000 (9570%)\n",
            "\n",
            "Train Epoch: 35 [0/10000 (0%)]\tLoss: 0.000043\n",
            "\n",
            "Test set: Average loss: 0.2251, Accuracy: 958/1000 (9580%)\n",
            "\n",
            "Train Epoch: 36 [0/10000 (0%)]\tLoss: 0.000040\n",
            "\n",
            "Test set: Average loss: 0.2255, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 37 [0/10000 (0%)]\tLoss: 0.000038\n",
            "\n",
            "Test set: Average loss: 0.2260, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 38 [0/10000 (0%)]\tLoss: 0.000037\n",
            "\n",
            "Test set: Average loss: 0.2264, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 39 [0/10000 (0%)]\tLoss: 0.000035\n",
            "\n",
            "Test set: Average loss: 0.2269, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 40 [0/10000 (0%)]\tLoss: 0.000033\n",
            "\n",
            "Test set: Average loss: 0.2273, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 41 [0/10000 (0%)]\tLoss: 0.000032\n",
            "\n",
            "Test set: Average loss: 0.2278, Accuracy: 959/1000 (9590%)\n",
            "\n",
            "Train Epoch: 42 [0/10000 (0%)]\tLoss: 0.000031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynf42c_UK9Fn",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    }
  ]
}